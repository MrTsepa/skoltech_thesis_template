%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction}

The development of modern robotic systems is impossible without advanced sensors aggregating information about the environment. With the growing accessibility of 3d sensors such as LIDARs and RGBD cameras, 3d point clouds become the most demanded source of information about the world for a robot. Many researchers focus on developing algorithms to extract meaningful information from point clouds. Such algorithms include semantic segmentation, localization using key points and objects, ground detection, object tracking, etc. Usually, these algorithms rely on convenient noise-free data obtained with simple filtering techniques \cite{dense_planar_slam}.

%%TODO
As an alternative to algorithms, relying on depth as a source of information, there are algorithms analyzing surface normals extracted from a depth image. One example of such algorithm is the segmentation of planar areas, where surface normals contain valuable information. In most cases, simple geometrical approach is used to estimate normals, for each pixel and its neighborhood best-fitting plane is chosen. The normal to that plane is used as a resulting normal estimation.  In such situation, several obstacles exist. Firstly, due to depth corruption, calculated normals may contain artifacts. Secondly, ambiguity exists on what scale normals should be estimated. If one uses small neighborhood size, the resulting normals will capture every small detail of input data including not only edges but also all noise and artifacts. Oppositely, if one uses a large neighbourhood size, the estimated normals are very smooth and lack of details. We will illustrate such ambiguity in chapter \ref{multiscale-normal-estimation}. Simply speaking, the scale is a hyperparameter in the task of estimating normals. In our work, we are building a supervised hyperparameter-free approach to normal estimation.

