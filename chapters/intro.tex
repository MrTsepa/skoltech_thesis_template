%% This is an example first chapter.  You should put chapter/appendix that you
%% write into a separate file, and add a line \include{yourfilename} to
%% main.tex, where `yourfilename.tex' is the name of the chapter/appendix file.
%% You can process specific files by typing their names in at the 
%% \files=
%% prompt when you run the file main.tex through LaTeX.
\chapter{Introduction}

Developement of modern robotic systems is impossible without using complex sensors to aggregate information about their environment. With growing accessibility of 3D sensors like LIDARs and RGBD cameras 3D point clouds are becoming the most demanded source of information about the world for a robot. Many researchers are developing algorithms to extract meaningful information from point clouds. Such algorithms include semantic segmentation, localization using key points and objects, ground detection, object tracking, etc. Usually these algorithms rely on convenient noise-free data. To achieve this simple filtering techniques are used \cite{dense_planar_slam}.

Instead of using depth as a source of information, some algorithms may rely on surface normals extracted from depth image. An example of such algorithm is segmentation of planar areas, where surface normals contain very valuable information. In most cases simple geometrical approach is used to estimate normals, for each pixel and its neighbourhood best fitting plane is choosed. The normal to that plane is used as resulting normal estimation.  In such situation several obstacles exist. Firstly, due to depth corruption calculated normals may contain artifacts. Secondly, ambiguity exists on what scale normals should be estimated. If one uses small neighbourhood size the resulting normals will capture every small detail of input data including not only edges but also all noise and artifacts. Oppositely, if one uses large neighbourhood size, estimated normals are very smooth and lack of details. We will illustrate such ambiguity in chapter \ref{method}. Simply speaking, scale is a hyperparameter in task of estimating normals. In our work we are building a supervised hyperparameter-free approach to normal estimation.

